{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling is a core skill that everyone who works with data should be familiar with since so much of the world's data isn't clean this process is divided to 3 steps:\n",
    "\n",
    "1.Gathering data\n",
    "\n",
    "2.Assesing data\n",
    "\n",
    "3.cleaning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "The dataset that I used in this project is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering data is the first step in data wrangling before it we don’t have any data after it we have\n",
    "This project involved gathering data from three different sources as listed below:\n",
    "\n",
    "* WeRateDogs Twitter archive. This contains 5000+ basic tweet data about dog rating, name, and \"stage\".\n",
    "\n",
    "\n",
    "* tweet image predictions from Udacity site. This file contains dog breed prediction results (from a Neural Network classifier)  for every dog images from the WeRateDogs Twitter archive.\n",
    "\n",
    "\n",
    "* Twitter API tweepy. Use this API to query additional data (in JSON format) for each tweet ID in the WeRateDogs Twitter archive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After gathering data the next step is assessing it visually and programmatically to detect quality and tidiness issues\n",
    "\n",
    "After my assessing of data I found some issues:\n",
    "\n",
    "quality\n",
    "\n",
    "##### `dogs_rates_archive` table\n",
    "\n",
    "•\tname column has none,a,an,.. values instead of NAN\n",
    "\n",
    "•\tthere are columns we don't need (in_reply_to_status_id, \n",
    "in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp)\n",
    "\n",
    "•\twrong data types(timestamp,tweet_id).\n",
    "\n",
    "•\tthere are retweeted tweets and we only want original tweeets\n",
    "\n",
    "•\trating_numerator and rating_denominator have wrong ratings, like 960/0 , 24/7, 9/11, 165/150\n",
    "\n",
    "##### `image_predictions` table\n",
    "\n",
    "•\tpredicition 1,2,3 column are write as abbrevation p1,2,3\n",
    "\n",
    "•\tthere are tweets with no images as image_predictions table has 2075 observations and the archive table has 2356 observations\n",
    "\n",
    "•\ttweet id is int64not object\n",
    "\n",
    "•\tthree predictions of the breed of dogs,but one of them is the most confident.\n",
    "\n",
    "##### `tweet_df` table\n",
    "\n",
    "change name of id column to tweet_id to be consistent\n",
    "\n",
    "tidines\n",
    "\n",
    "•\tdog stages is one variable but in 4 columns\n",
    "\n",
    "•\tthree datasets instead of one .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning  data is the third step in data wrangling. It is the process of fixing the quality and tidiness issues that we identified in the assess step,to make sure the the data is accurate and cleaning. And then being able to analyze our data \n",
    "\n",
    "Before starting the cleaning process I make a copy of each data set I have, then I tried to correct the issues identified in assessing step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
