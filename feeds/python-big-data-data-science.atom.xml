<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Mohamed Abdo - Python, Big Data, Data Science</title><link href="http://mabdo250.github.io/" rel="alternate"></link><link href="http://mabdo250.github.io/feeds/python-big-data-data-science.atom.xml" rel="self"></link><id>http://mabdo250.github.io/</id><updated>2017-08-17T00:00:00+02:00</updated><entry><title>How to configure an Apache Spark standalone cluster and integrate with Jupyter: Step-by-Step</title><link href="http://mabdo250.github.io/python-big-data-data-science/how-to-spark-cluster.html" rel="alternate"></link><published>2017-08-17T00:00:00+02:00</published><updated>2017-08-17T00:00:00+02:00</updated><author><name>David Adrián Cañones Castellano</name></author><id>tag:mabdo250.github.io,2017-08-17:/python-big-data-data-science/how-to-spark-cluster.html</id><summary type="html">&lt;p&gt;Learn how to create and configure your Spark cluster and set up Jupyter notebook PySpark&amp;nbsp;integration.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;What is&amp;nbsp;Spark?&lt;/h2&gt;
&lt;p&gt;Spark is a framework to make computations with large amounts of data. Why do you need something like Spark? Think for
example about a small dataset that fit easily into memory, let&amp;#8217;s say some Gb maximum. You will probably load the entire 
dataframe using Pandas, R or your tool of choice and after some quick cleaning and visualization you will be almost done
 with no major hassles related with computing performance if you are using a proper computer (or cloud&amp;nbsp;infrastructure).  &lt;/p&gt;
&lt;p&gt;Now think that you have to process a 1Tb (or bigger) dataset and train a &lt;span class="caps"&gt;ML&lt;/span&gt; algorithm on it. Even with a powerful computer 
it is crazy. Spark gives you two features you need to handle these data&amp;nbsp;monsters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Parallel computing&lt;/strong&gt;: you use not one but many computers to speed your&amp;nbsp;calculations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault tolerance&lt;/strong&gt;: you must be able to recover if one of your computers hangs in the middle of the&amp;nbsp;process.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-overview.html"&gt;How Spark works internally&lt;/a&gt; is out 
of the scope of this tutorial and I will asume you are already familiar with that. Anyway you will need little 
knowledge about Spark&amp;#8217;s internals to set up and run you own cluster at&amp;nbsp;home.&lt;/p&gt;
&lt;h2&gt;What is a Spark cluster and what does &amp;#8216;standalone&amp;#8217;&amp;nbsp;mean?&lt;/h2&gt;
&lt;h3&gt;Spark&amp;nbsp;clusters&lt;/h3&gt;
&lt;p&gt;A Spark cluster is just some computers running Spark and working together. A cluster consist&amp;nbsp;on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Master&lt;/strong&gt;: is one of the computers that orchestrate how everything works. It distributes the work and take care of&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Slaves&lt;/strong&gt;: these are the computers that get the job done. They process chunks of your massive datasets following the Map 
Reduce paradigm. A computer can be master and slave at the same&amp;nbsp;time.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Standalone&lt;/h3&gt;
&lt;p&gt;It just mean that Spark is installed in every computer involved in the cluster. The cluster manager in use is provided 
by Spark. There are other cluster managers like Apache Mesos and Hadoop &lt;span class="caps"&gt;YARN&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;p&gt;To follow this tutorial you&amp;nbsp;need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A couple of computers (minimum): this is a&amp;nbsp;cluster.&lt;/li&gt;
&lt;li&gt;Linux: it should also work for &lt;span class="caps"&gt;OSX&lt;/span&gt;, you have to be able to run shell scripts. I have not seen Spark running on native 
windows so&amp;nbsp;far.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this tutorial I have used a MacBook Air with Ubuntu 17.04 and my desktop system with Windows 10 running
 &lt;a href="https://msdn.microsoft.com/es-es/commandline/wsl/install_guide"&gt;Linux Subsystem for Windows&lt;/a&gt; (yeah!) with Ubuntu 16.04 &lt;span class="caps"&gt;LTS&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If you don&amp;#8217;t meet these simple requirements, please don&amp;#8217;t panic, follow this steps and you are&amp;nbsp;done:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download &lt;a href="https://www.virtualbox.org/"&gt;Oracle Virtualbox&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Download &lt;a href="https://www.ubuntu.com/download/desktop"&gt;Linux&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.psychocats.net/ubuntu/virtualbox"&gt;Create a Virtual Machine in Virtualbox and install Linux on it&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://protechgurus.com/clone-virtual-machine-virtualbox/"&gt;Clone that &lt;span class="caps"&gt;VM&lt;/span&gt;&lt;/a&gt; after following the installation tutorial&amp;nbsp;steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that&amp;#8217;s all, you have 2 Linux machines to run your&amp;nbsp;cluster.&lt;/p&gt;
&lt;div class="alert alert-warning alert-dismissible"&gt;
    &lt;a href="#" class="close" data-dismiss="alert" aria-label="close"&gt;&amp;times;&lt;/a&gt;
    &lt;strong&gt;Warning&lt;/strong&gt;: using 2 virtual machines running on the same computer is &lt;span class="caps"&gt;OK&lt;/span&gt; for learning and prototyping
     purposes but you will lose any performance improvement you would have using a real cluster!
&lt;/div&gt;

&lt;h2&gt;Tutorial&lt;/h2&gt;
&lt;h3&gt;Step 1: Install&amp;nbsp;Java&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; you will have to perform this step for all machines&amp;nbsp;involved.&lt;/p&gt;
&lt;p&gt;Spark needs Java to run. My recommendation is going with Open &lt;span class="caps"&gt;JDK8&lt;/span&gt;. Go to your Terminal and write the following&amp;nbsp;commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get update
$ sudo apt-get upgrade
$ sudo apt-get install openjdk-8-jdk
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Test your Java installation&amp;nbsp;typing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ java -version
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see the following&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;openjdk version &amp;quot;1.8.0_131&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;OpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-2ubuntu1.17.04.3-b11)&lt;/span&gt;
&lt;span class="err"&gt;OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Step 2: Install&amp;nbsp;Spark&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz"&gt;Download&lt;/a&gt; Spark. I have used 2.2.0 pre-built for 
this tutorial. You can go to &lt;a href="https://spark.apache.org/downloads.html"&gt;Spark download page&lt;/a&gt; and download it from there 
or in case you don&amp;#8217;t have access to a graphical desktop and have to use command line just&amp;nbsp;run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -O https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After that you will have the compressed source of Spark in your home directory. Uncompress the code and move the resulting
folder to your home (recommended but not necessary) in case you downloaded it in Downloads&amp;nbsp;folder.&lt;/p&gt;
&lt;p&gt;Uncompress:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tar zxvf spark-2.2.0-bin-hadoop2.7.tgz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And move&amp;nbsp;(optional):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ mv spark-2.2.0-bin-hadoop2.7 ~
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally you will set the needed environment variable &lt;code&gt;SPARK_HOME&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Go to your home directory and open with a text editor the &amp;#8216;.bashrc&amp;#8217;&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; ~
$ sudo nano .bashrc
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Add the following lines at the end of the file. It will set up the variable properly when your turn on your computer or
start a session. Take into account that folder name will change in the future and will not match the one written here, 
but the procedure is&amp;nbsp;similar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Spark
export SPARK_HOME=&amp;quot;/home/&amp;lt;your_username&amp;gt;/spark-2.2.0-bin-hadoop2.7/&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Save the file. If you are using nano just do &lt;kbd&gt;ctrl&lt;/kbd&gt;+&lt;kbd&gt;x&lt;/kbd&gt;, write &lt;code&gt;y&lt;/code&gt; and press &lt;kbd&gt;return&lt;/kbd&gt; to get 
it&amp;nbsp;done.&lt;/p&gt;
&lt;p&gt;Now you can check your Spark installation. Go to Spark folder and execute&amp;nbsp;pyspark:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; spark-2.2.0-bin-hadoop2.7
$ bin/pyspark
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If everything is properly installed you should see an output similar to&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Python 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:09:58) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
Using Spark&amp;#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &amp;quot;WARN&amp;quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/08/17 20:39:03 WARN Utils: Your hostname, david-MacBookAir resolves to a loopback address: 127.0.1.1; using 192.168.1.143 instead (on interface wlp2s0)
17/08/17 20:39:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/08/17 20:39:13 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/08/17 20:39:13 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/08/17 20:39:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &amp;#39;_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/

Using Python version 3.6.1 (default, May 11 2017 13:09:58)
SparkSession available as &amp;#39;spark&amp;#39;.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Up to this point, you may see this warning at Spark&amp;nbsp;initialization:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&amp;quot;.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To solve this problem, we will have to install Hadoop. This is optional because Spark is going to run anyway, but I guess 
there must be some performance improvements of using native Hadoop over some kind of&amp;nbsp;adapters.&lt;/p&gt;
&lt;h3&gt;Step 3 (Optional): Installing&amp;nbsp;Hadoop&lt;/h3&gt;
&lt;p&gt;The process of installing Hadoop is pretty much the same as Spark, and I will go over it quickly. This is an optional step 
and is not strictly necessary to run your cluster. You can also decide to do it later. The steps for installing Hadoop&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download Hadoop source &lt;a href="http://apache.rediris.es/hadoop/common/stable/"&gt;here&lt;/a&gt;. You are free 
to install whatever version you like, but make sure you choose a version &amp;gt; 2.7 because it&amp;#8217;s a Spark&amp;#8217;s requirement. This
  is pretty much the same as we did before with Spark. Feel free to use equivalent command line&amp;nbsp;expressions.&lt;/li&gt;
&lt;li&gt;Uncompress in your home&amp;nbsp;directory.&lt;/li&gt;
&lt;li&gt;Add the following lines at the end of your &lt;code&gt;.bashrc&lt;/code&gt; file: &lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Hadoop
export HADOOP_HOME=&amp;quot;/home/&amp;lt;your_username&amp;gt;/hadoop-2.8.0&amp;quot;
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Test your Hadoop installation by starting pyspark and make sure you don&amp;#8217;t see the&amp;nbsp;warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 4: Launch the master&amp;nbsp;server&lt;/h3&gt;
&lt;p&gt;Previous to launch the master server, you must check your ip to pass it as the host parameter. Type the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ifconfig
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see an output similar to&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;lo: flags=73&amp;lt;UP,LOOPBACK,RUNNING&amp;gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10&amp;lt;host&amp;gt;
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 3735  bytes 7174785 (7.1 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 3735  bytes 7174785 (7.1 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

wlp2s0: flags=4163&amp;lt;UP,BROADCAST,RUNNING,MULTICAST&amp;gt;  mtu 1500
        inet 192.168.1.143  netmask xxx.xxx.xxx.x  broadcast xxx.xxx.x.xxx
        inet6 xxxx::xxxx:xxxx:xxxx:xxxx  prefixlen 64  scopeid 0x20&amp;lt;link&amp;gt;
        ether 14:10:9f:f3:a0:5e  txqueuelen 1000  (Ethernet)
        RX packets 67543  bytes 60399180 (60.3 MB)
        RX errors 0  dropped 0  overruns 0  frame 119733
        TX packets 52684  bytes 11712399 (11.7 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        device interrupt 17  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copy the &lt;code&gt;inet&lt;/code&gt; value inthe second text block. This is going to be &lt;code&gt;&amp;lt;your_master_ip&amp;gt;&lt;/code&gt;. In this example you should copy &lt;code&gt;192.168.1.143&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, go to your Spark installation directory and&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./sbin/start-master.sh -h &amp;lt;your_master_ip_&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see an output similar&amp;nbsp;to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;starting org.apache.spark.deploy.master.Master, logging to /home/&amp;lt;your_username&amp;gt;/spark-2.2.0-bin-hadoop2.7//logs/spark-&amp;lt;your_username&amp;gt;-org.apache.spark.deploy.master.Master-1-&amp;lt;your_username&amp;gt;-MacBookAir.out
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you can go to &lt;code&gt;localhost:8080&lt;/code&gt; in the same computer where you started the master server, or to &lt;code&gt;&amp;lt;your_master_ip&amp;gt;:8080&lt;/code&gt; in 
case you don&amp;#8217;t have access to a web browser but have other devices connected to the same private network (e.g. a phone).
In that &lt;span class="caps"&gt;URL&lt;/span&gt; you have access to the master server web user&amp;nbsp;interface.&lt;/p&gt;
&lt;p&gt;&lt;img alt="master web ui" src="{static}/images/posts/spark_cluster/master_web_ui.png" title="Master Web UI"&gt;&lt;/p&gt;
&lt;p&gt;In this web you can see just behind Spark logo an &lt;span class="caps"&gt;URL&lt;/span&gt; parameter similar to &lt;code&gt;spark://&amp;lt;your_master_ip&amp;gt;:7077&lt;/code&gt;. This &lt;span class="caps"&gt;URL&lt;/span&gt; is 
very important because is the one you are going to need when connecting slaves to your cluster and I will name it &lt;code&gt;&amp;lt;your_master_url&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Step 5: Connect your&amp;nbsp;slaves&lt;/h3&gt;
&lt;p&gt;Now you have a master server running, is time to start a couple of slave servers to get the job done. To start a slave 
server you have to type the following command running from your Spark installation folder, using the &lt;span class="caps"&gt;URL&lt;/span&gt; you copied from 
master server web&amp;nbsp;interface:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./sbin/start-slave.sh &amp;lt;your_master_url&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see an output very similar to the master&amp;#8217;s&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;Perform this operation in every computer you want to connect to the cluster. In this tutorial I am going to run 2&amp;nbsp;slaves.&lt;/p&gt;
&lt;p&gt;After that, if you go to the master server web interface again you should see several rows in the &amp;#8216;Workers&amp;#8217; section where, 
one per slave server you started. There is also useful info like worker&amp;#8217;s memory, status, cores used and&amp;nbsp;ip.&lt;/p&gt;
&lt;p&gt;&lt;img alt="master slaves_web ui" src="{static}/images/posts/spark_cluster/master_slave_web_ui.png" title="Master Web UI with Workers"&gt;&lt;/p&gt;
&lt;p&gt;You can also click the links in the Spark web app and go to worker&amp;#8217;s page and tasks page. As you don&amp;#8217;t have any running 
app connected to the cluster, you can&amp;#8217;t access running and completed tasks&amp;nbsp;page.&lt;/p&gt;
&lt;p&gt;So far you have a fully working Spark cluster&amp;nbsp;running.&lt;/p&gt;
&lt;h3&gt;Step 6: Integration with&amp;nbsp;Jupyter&lt;/h3&gt;
&lt;p&gt;To get the most out of Spark is a good idea integrating with some interactive tool like Jupyter. If you already have 
Jupyter installed and running, skip the following lines where I explain how to set up a local Jupyter&amp;nbsp;server.&lt;/p&gt;
&lt;h4&gt;Installing&amp;nbsp;Jupyter&lt;/h4&gt;
&lt;p&gt;The easiest way to install Jupyter is probably using &lt;a href="https://conda.io/docs/intro.html"&gt;conda&lt;/a&gt; 
(package, dependency and environment management). If you have &lt;a href="https://www.continuum.io/downloads"&gt;Anaconda Python distribution&lt;/a&gt;, 
conda is already installed in your computer. If not, I strongly recommend you giving Anaconda a try, it really worth it. 
In case you don&amp;#8217;t want to install the full Anaconda Python (it includes a lot of libraries and needs about 350 Mb of disk) 
you can opt for Miniconda, a lighter version which only includes Python and&amp;nbsp;conda.&lt;/p&gt;
&lt;p&gt;If you are using command line, just download the installation file (shell script) using curl and execute it with &lt;code&gt;'./'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Once you have conda installed in your computer, let&amp;#8217;s create a conda virtual environment called &lt;code&gt;jupyter&lt;/code&gt; to avoid 
messing the root&amp;nbsp;one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ conda create -n jupyter
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now have a clean virtual environment to install Jupyter on it. To activate this environment&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;source&lt;/span&gt; activate jupyter
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the environment activated you should see &lt;code&gt;(jupyter)&lt;/code&gt; at the beginning of your command prompt. Something like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;(jupyter) david@david-MacBookAir:~$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It means that your environment is activated and all changes you do hereinafter (installing libraries, etc.) will be 
applied to this conda virtual&amp;nbsp;environment.&lt;/p&gt;
&lt;p&gt;To install Jupyter, type the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;(jupyter) $ conda install notebook&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command will install Jupyter notebook and all its dependencies (IPython, Jupyter, etc.) so you don&amp;#8217;t have to worry 
about setting all these things up (thanks to conda package&amp;nbsp;manager!).&lt;/p&gt;
&lt;p&gt;Now it&amp;#8217;s time to launch a Jupyter notebook and test your installation.&amp;nbsp;Type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;(jupyter) $ jupyter notebook&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If Jupyter is properly installed you should be able to go &lt;code&gt;localhost:8888/tree&lt;/code&gt; &lt;span class="caps"&gt;URL&lt;/span&gt; in a web browser and see Jupyter folder&amp;nbsp;tree.&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyter_notebook_tree" src="{static}/images/posts/spark_cluster/jupyter_notebook_tree.png" title="Jupyter Notebook Folder Tree"&gt;&lt;/p&gt;
&lt;h4&gt;Installing&amp;nbsp;findspark&lt;/h4&gt;
&lt;p&gt;findspark is a Python library that automatically allow you to import and use PySpark as any other Python library. There 
are other options to make the integration (create a jupyter profile for Spark) but up to date findspark is imho the 
faster and simpler&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;To install findspark run the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;(jupyter) $ pip install findspark&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you have findpark installed in your jupyter virtual environment. Let&amp;#8217;s create your first&amp;nbsp;application.&lt;/p&gt;
&lt;h4&gt;Create your first Spark&amp;nbsp;application&lt;/h4&gt;
&lt;p&gt;To create your first Spark app and start to make cool things with data, run the following script in a Jupyter&amp;nbsp;cell:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;findspark&lt;/span&gt;

&lt;span class="n"&gt;findspark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt;

&lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyspark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;master&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;your_master_url&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;your_app_name&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This script just initialize findspark so you can import PySpark as a library, and create an instance of Spark Context with 
your master &lt;span class="caps"&gt;URL&lt;/span&gt; and app name (up to you) as&amp;nbsp;parameters.&lt;/p&gt;
&lt;p&gt;To test everything works well, you can display sc in your Jupyter notebook and should see an output like&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyter_notebook_spark_context" src="{static}/images/posts/spark_cluster/jupyter_notebook_spark_context.png" title="Jupyter Notebook Spark Context"&gt;&lt;/p&gt;
&lt;p&gt;You can click the link and go to your app web ui, which is very interesting while you a running long&amp;nbsp;tasks.&lt;/p&gt;
&lt;p&gt;You should also connect to your master server web interface (&lt;code&gt;localhost:8080&lt;/code&gt; or &lt;code&gt;&amp;lt;your_master_ip&amp;gt;:8080&lt;/code&gt;) and see a new row 
in Running Applications section, like&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="master_app_web_ui" src="{static}/images/posts/spark_cluster/master_app_web_ui.png" title="Master Web UI with Spark Application"&gt;&lt;/p&gt;
&lt;p&gt;To turn off your master and slaves servers, use the following commands from your Spark installation&amp;nbsp;directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./sbin/stop-master.sh
$ ./sbin/stop-slave.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To deactivate your jupyter conda virtual environment, just run the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;(jupyter) $ source deactivate jupyter&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And that&amp;#8217;s&amp;nbsp;all!&lt;/p&gt;
&lt;p&gt;There are many other things you can tweak and configure about Spark! If people is interested, I will create more content 
about this amazing technology in the future as I am still learning new concepts&amp;nbsp;everyday.&lt;/p&gt;
&lt;p&gt;Hope you found this tutorial useful. If you see any error, bug, errand or whatever, just tell me or leave a comment and I 
will try to fix it&amp;nbsp;asap.&lt;/p&gt;</content><category term="Apache Spark"></category><category term="Python"></category><category term="Big Data"></category><category term="Jupyter"></category><category term="Data Science"></category></entry></feed>